---
title: "Support vector machines"
date: 2020-06-16
categories: [Machine learning algorithms]
excerpt: "A thorough explanation of the one of the best off-the-shelf machine learning aglorithms: support vector machine."
header: 
 image: "/images/SVM-image.jpg"
mathjax: "true"
classes: wide
sidebar:
  title: "Support vector machines"
  nav: sidebar-SVM
---

## 1. Decision boundary and primal form

The support vector machine (or, abbreviated, "SVM") is a numerical classifier. It is one of the best “off-the-shelf” supervised learning algorithms because its practical simplicity and computational efficiency (compared to, for instance, artificial neural networks). Given a training set  \\(S = \\{(x^{(i)},y^{(i)})\\}\\) with \\(i = 1 \space , ..., \space m\\), where each example belongs to either a positive or negative class i.e. \\(y^{(i)} \in \\{-1, +1\\}\\), a SVM training algorithm builds a model that assigns new examples to one category or the other. It does so by means of a linear decision boundary. This linear decision boundary is defined by a line in 2D, a plane in 3D or a hyper plane in higher dimensional feature space, as follows:

\begin{align}
f(x) = w^{T}x+b = 0 \Leftrightarrow \sum_{i = 1}^{n} w_{i}x_{i} + b = 0 \tag{1-1}
\end{align}

where \\(n\\) is the number of features in our dataset, \\(x \in \mathcal{R}^n\\) is a column vector of feature values,  \\(w \in \mathcal{R}^n\\) is a column vector of weights that is normal to the separating hyperplane and \\(b \in \mathcal{R}\\) is the bias term, without it, the classifier will always go through the origin.

Clearly, a new set of feature values \\( x^{\*} \\) lays on the linear decision boundary if it satisfies Equation (1-1). Similarly, it will lay above the decision boundary if \\(f(x^{\*}) > 0\\) and below the decision boundary if \\(f(x^{\*})<0\\). Consequently, the prediction rule, denoted by \\(h(x^{\*})\\), is given by the sign of \\(f(x^{\*})\\). That is, a training example is classified as either a positive or a negative class, as such:

\begin{align}
h(x^{\*})= \tag{1-2}
\begin{cases}
1 & \quad \text{if } w^{T}x^{\*}+b \geq 0 \newline
-1 & \quad \text{if }w^{T}x^{\*}+b < 0 \newline
\end{cases}
\end{align}

For now, we assume that the data is linearly seperatable.* In such an environment, many possible linear seperators exist. Figure 1 provides two different kinds of linear seperators, dividing the positive class, denoted with crosses, from the negative class, denoted with circles.

(\*) _We relax this assumption once we introduce the kernelized SVM._

__Figure 1: Different possible linear seperators__

![Figure 1: Different possible linear seperators](/images/possible-decision-boundaries.png)
  
The SVM, however, has a special kind of decision boundary. In contrast to the decision boundaries demonstrated in Fig. 1, the SVM assures that the decision boundary is maximally far away from any data point. The distance from the decision boundary to the closest data point determines the so-called __margin__ of the classifier, as shown in Figure 2.

__Figure 2: The maximum margin decision boundary of a SVM__

![Figure 2: The maximum margin decision boundary of a SVM](/images/maximum-margin-decision-boundary.png)

Intuitively, a classifier with a larger margin makes fewer low certainty classification decisions. That is, if we are asked to make a class prediction of a training example close to the decision boundary, this would reflect an uncertain decision. In particular, a small distortion of the feature values could change the outcome of our prediction. By maximizing the margin, we reduce the need to make these kind of low certainty decisions and provide ourselves with some sort of safety margin.

### 1.1 Functional margin
Given a training example \\((x^{(i)}, y^{(i)})\\),  we define the functional margin with respect to the training example as:

\begin{align}
\hat{\gamma}^{(i)} = y^{(i)}(w^{T}x^{(i)}+b) \tag{1-3}
\end{align}

Note that, according to the decision rule of Equation (1-2), if \\(y^{(i)} = 1\\) then we need \\(w^{T}x^{(i)}+b\\) to be positive in order for the prediction to be correct. At the same time, we need \\(w^{T}x^{(i)}+b\\) to be large for our prediction to be confident (i.e. far away from the decision surface). Similarly, if \\(y^{(i)} = -1\\), we need \\(w^{T}x^{(i)}+b\\) to be a large negative number for our prediction to be confident and correct. Hence, when \\(\hat{\gamma}^{(i)}\\) is large and positive this represents a correct and confident prediction.

Note, however, that the functional margin is underconstrained. That is, if we replace \\(w\\) with \\(2w\\) and \\(b\\) with \\(2b\\), the decision surface nor the sign of the output will change. To elaborate, if we scale our parameters with a constant \\(c\\), the function \\(c (w^{T}x+b) = 0\\) produces the exact same linear decision surface as \\(w^{T}x+b = 0\\) ([desmos](https://www.desmos.com/calculator/lsy0fpi94g)). Thus, the decision rule is unaffected by scaling of the parameters. However, by replacing \\((w,b)\\) with \\((2w,2b)\\) the functional margin does get multiplied by a factor of 2. This implies that we can make the functional margin as large as we please by simply exploiting our freedom to scale \\(w\\) and \\(b\\), without affecting the actual decision boundary. We will exploit this property later on, when simplifying the optimization objective.

We also define the functional margin with respect to the training set \\(S\\) as the smallest of the functional margins of the individual training examples. Denoted by \\(\hat{\gamma}\\), this can therefore be written:

\begin{align}
\hat{\gamma} = \min_{i = 1,...,m} \hat{\gamma}^{(i)} \tag{1-4}
\end{align}

### 1.2 Geometric margin
To define the geometric margin, we look at the Euclidean distance from any point \\(\vec{x}^{(i)}\\) of the training set \\(S\\) to the linear decision boundary. We know that the shortest distance between a point and a hyperplane is perpendicular (at 90°) to the plane. This perpendicular vector, pointing between \\(\vec{x}^{(i)}\\) and the decision boundary, is denoted by \\(\vec{r}^{(i)}\\) in Fig. 3. To stay consistent with our notation, we denote the magnitude of this vector as \\(\gamma^{(i)}\\) ( i.e. \\(\|\vec{r}^{(i)}\| = \gamma^{(i)}\\)). Now, taken into account that the normal vector \\(\vec{w}\\) is also perpendicular to the plane, \\(\vec{r}^{(i)}\\) and \\(\vec{w}\\) have the exact same direction. Consequently, we can the express the vector \\(\vec{r}^{(i)}\\) as:

\begin{align}
\vec{r}^{(i)} = \gamma^{(i)} \hat{w} \tag{1-5}
\end{align}

where \\(\gamma^{(i)}\\) and the unit normal vector \\(\hat{w}\\) capture the correct magnitude and direction of the vector \\(\vec{r}^{(i)}\\), respectively.

__Figure 3: The normal \\(\hat{w}\\) and \\(\vec{r}\\) are parallel__

![Figure 3: The normal vector to the decision boundary and the vector r are parallel](/images/derivation-of-margin.png)

Let us label the point on the hyperplane closest to \\(\vec{x}^{(i)}\\) as \\(\vec{x}^{(i)}_{P}\\). Following the rules of vector addition we get,

\begin{align}
\vec{x}^{(i)}_P = \vec{x}^{(i)} - y^{(i)} \* \vec{r}^{(i)} \tag{1-6}\\
\end{align}

Note that we multiply \\(\vec{r}^{(i)}\\) by \\(y^{(i)} \in \{1,-1\}\\) to capture the desired sign of the operation. In particular, \\(\vec{x}^{(i)}_{P} = \vec{x}^{(i)} + \vec{r}^{(i)}\\) if \\(\vec{x}^{(i)}\\) lays below the decision boundary and \\(\vec{x}^{(i)}_P = \vec{x}^{(i)} - \vec{r}^{(i)}\\) if it lays above the plane. 

Now, substituting (1-5) into (1-6) and writing out the unit normal gives,

\begin{align}
\vec{x}^{(i)}_P = \vec{x}^{(i)} - y^{(i)} \* \gamma^{(i)}  \* {\frac{\vec{w}}{\|\vec{w}\|}} \tag{1-7}\\
\end{align}

Moreover, \\(\vec{x}^{(i)}_P\\) lies on the decision boundary and so satisfies Equation (1-1). Substititon therefore yields,

\begin{align}
\vec{w}^{T} \* (\vec{x}^{(i)} - y^{(i)} \* \gamma^{(i)} \* {\frac{\vec{w}}{\|\vec{w}\|}})+b=0 \tag{1-8}\\
\end{align}

Multiplying out the brackets of Equation (1-8) yields,

\begin{align}
\vec{w}^{T} \vec{x}^{(i)} - y^{(i)} \* \gamma^{(i)}  \* \frac{\vec{w}^{T}\vec{w}}{\|\vec{w}\|}+b=0 \tag{1-9}\\
\end{align}

Substituting \\(\vec{w}^{T}\vec{w} = \|\vec{w}\|^{2}\\) into Equation (1-9) and simplifying gives,

\begin{align}
\vec{w}^{T} \vec{x}^{(i)} - y^{(i)} \* \gamma^{(i)} \* \|\vec{w}\|+b=0 \tag{1-10}\\
\end{align}

Finally, solving Equation (1-10) for \\(\gamma^{(i)}\\) gives the geometric margin of an individual training example,

\begin{align}
\gamma^{(i)} =  y^{(i)} \frac{\vec{w}^{T}\vec{x}^{(i)}+b}{\|\vec{w}\|} \tag{1-11}\\
\end{align}

Clearly, we cannot maximize the margin with respect to every training point. Instead, we maximize the margin with respect to the closest points to the line. If we define \\(\gamma\\) as the distance between the decision boundary and the point closest to it, then

\begin{align}
\gamma = \min_{i = 1, ..., m}\gamma^{(i)} \tag{1-12}\\
\end{align}

From this, we find the values for the parameters \\(w\\) and \\(b\\) which assure that \\(\gamma\\) is as big as possible. That is, we can extract the optimal geometric margin classifier as:

\begin{equation}
\begin{array}{rrclcl}
\displaystyle \max_{w, b} && \gamma \tag{1-13} \newline
\textrm{subject to} & \text{ } & y^{(i)} \frac{\vec{w}^{T}\vec{x}^{(i)}+b}{\|\vec{w}\|} \geq \gamma & \forall i = 1, ..., m \newline
\end{array}
\end{equation}

That is, we find the values for the parameters \\(w\\) and \\(b\\) which assure that \\(\gamma\\) is as big as possible, subject to the fact that the distance from the decision boundary to every training example is larger than or equal to the point closest to it. Note that, because we multiply by \\(y^{(i)}\\) in Equation (1-6), this maximization procedure forces the training examples to be correctly labelled (i.e. misclassifications will produce negative values of \\(\gamma\\), which opposes the optimization objective). The points closest to the separating hyperplane are called the ___support vectors___. The optimal geometric margin classifier of Equation (1-13) produces the margin with the maximum width of the band that can be drawn separating the support vectors of the two classes.

Note that the geometric margin is invariant to rescaling of the parameters; i.e., if we replace \\((w, b)\\) with \\((2w, 2b)\\), then the geometric margin does not change. This is due to the normalization parameter in the denominator. This will come in handy later. Specifically, because of this invariance to the scaling of the parameters, when trying to fit \\(w\\) and \\(b\\) to training data, we can impose an arbitrary scaling constraint on the parameters without changing anything important.

### 1.3 Primal form
The above optimization has a non-convex constraint, which we cannot plug into standard optimization software to solve. Instead, we consider an alternative:

\begin{align}
\displaystyle \max_{w, b}  && \hat{\gamma}/\|w\|  \tag{1-14} \newline
\textrm{subject to} & text{ } & y^{(i)}(w^{T}x^{(i)} + b) \geq \hat{\gamma} & \forall i = 1, ..., m \newline
\end{align}

Here, we are maximizing \\(\hat{\gamma}/\|w\|\\), subject to the functional margins being at least \\(\hat{\gamma}\\). This is equivalent to the earlier optimization since the geometric and functional margin are related by \\(\gamma = \hat{\gamma}/\|w\|\\). This allow us to get rid of the non-convex constraint. However, we still have a nasty non-convex objective function. Now, taken into account that we can add an arbitrary scaling constraint on \\(w\\) and \\(b\\) without changing anything important, we will introduce the scaling constraint that the functional margin of \\(w\\), \\(b\\) with respect to the training set must be 1:

\begin{align}
\hat{\gamma} = 1 \tag{1-15}\\
\end{align}

Since multiplying \\(w\\) and \\(b\\) by some constant \\(c\\) results in the functional margin being multiplied by that same constant, this is indeed a scaling constraint, and can be satisfied by rescaling \\(w\\), \\(b\\). Per our earlier discussion, this leaves both the geometric margin and decision boundary unaffected. Thus, plugging this into our problem above, and noting that maximizing \\(1/\|w\|\\) is the same thing as minimizing \\(\|w\|^{2}\\), we now have the following optimization problem:

\begin{equation}
\begin{array}{rrclcl}
\displaystyle \min_{w, b} && \frac{1}{2} \|w\|^{2} \tag{1-16} \newline
\textrm{subject to} & \text{ } & y^{(i)} (w^{T}x^{(i)} + b) \geq 1 & \forall i = 1, ..., m \newline
\end{array}
\end{equation}

The above is an optimization problem with a convex quadratic objective and only linear constraints. Its solution gives us the optimal margin classifier and can be obtained using quadratic programming.