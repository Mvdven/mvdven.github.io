---
title: "Support vector machines"
date: 2020-06-16
categories: [Machine learning algorithms]
excerpt: "A thorough explanation of the one of the best off-the-shelf machine learning aglorithms: support vector machine."
header: 
 image: "/images/SVM-image.jpg"
comments: true
mathjax: "true"
classes: wide
sidebar:
  title: "Support vector machines"
  nav: sidebar-SVM
---

<font size="6"><b>  Chapter 1: The maximum margin classifier </b></font>

### 1.1 Decision boundary
The support vector machine (or, abbreviated, "SVM") is a numerical classifier. It is one of the best “off-the-shelf” supervised learning algorithms because its practical simplicity and computational efficiency (compared to, for instance, artificial neural networks). Given a training set  \\(S = \\{(x^{(i)},y^{(i)})\\}\\) with \\(i = 1 \space , ..., \space m\\), where each example belongs to either a positive or negative class i.e. \\(y^{(i)} \in \\{-1, +1\\}\\), a SVM training algorithm builds a model that assigns new examples to one category or the other. It does so by means of a linear decision boundary. This linear decision boundary is defined by a line in 2D, a plane in 3D or a hyper plane in higher dimensional feature space, as follows:

\begin{align}
f(x) = w^{T}x+b = 0 \Leftrightarrow \sum_{i = 1}^{n} w_{i}x_{i} + b = 0 \tag{1-1} \label{1-1}
\end{align}

where \\(n\\) is the number of features in our dataset, \\(x \in \mathcal{R}^n\\) is a column vector of feature values,  \\(w \in \mathcal{R}^n\\) is a column vector of weights that is normal to the separating hyperplane and \\(b \in \mathcal{R}\\) is the bias term, without it, the classifier will always go through the origin.

Clearly, a new set of feature values \\( x^{\*} \\) lays on the linear decision boundary if it satisfies Equation \eqref{1-1}. Similarly, it will lay above the decision boundary if \\(f(x^{\*}) > 0\\) and below the decision boundary if \\(f(x^{\*})<0\\). Consequently, the prediction rule, denoted by \\(h(x^{\*})\\), is given by the sign of \\(f(x^{\*})\\). That is, a training example is classified as either a positive or a negative class, as such:

\begin{align}
h(x^{\*})= \tag{1-2} \label{1-2}
\begin{cases}
1 & \quad \text{if } w^{T}x^{\*}+b \geq 0 \newline
-1 & \quad \text{if }w^{T}x^{\*}+b < 0 \newline
\end{cases}
\end{align}

For now, we assume that the data is linearly seperatable.* In such an environment, many possible linear seperators exist. Figure 1 provides two different kinds of linear seperators, dividing the positive class, denoted with crosses, from the negative class, denoted with circles.

\* _We relax this assumption once we introduce the kernelized SVM._

__Figure 1: Different possible linear seperators__

![Figure 1: Different possible linear seperators](/images/possible-decision-boundaries.png)
  
The SVM, however, has a special kind of decision boundary. In contrast to the decision boundaries demonstrated in Fig. 1, the SVM assures that the decision boundary is maximally far away from any data point. The distance from the decision boundary to the closest data point determines the so-called __margin__ of the classifier, as shown in Figure 2.

__Figure 2: The maximum margin decision boundary of a SVM__

![Figure 2: The maximum margin decision boundary of a SVM](/images/maximum-margin-decision-boundary.png)

Intuitively, a classifier with a larger margin makes fewer low certainty classification decisions. That is, if we are asked to make a class prediction of a training example close to the decision boundary, this would reflect an uncertain decision. In particular, a small distortion of the feature values could change the outcome of our prediction. By maximizing the margin, we reduce the need to make these kind of low certainty decisions and provide ourselves with some sort of safety margin.

To achieve the goal of maximizing the margin, we first introduce two concepts: the functional margin and the geometric margin. While the latter provides us with a valid optimization objective, the former allows us to simplify it. 

### 1.2 Functional margin
Given a training example \\((x^{(i)}, y^{(i)})\\),  we define the functional margin with respect to the training example as:

\begin{align}
\hat{\gamma}^{(i)} = y^{(i)}(w^{T}x^{(i)}+b) \Leftrightarrow y^{(i)}f(x^{(i)}) \tag{1-3} \label{1-3}
\end{align}

Note that, according to the decision rule of Equation \eqref{1-2}, if \\(y^{(i)} = 1\\) then we need \\(f(x^{(i)})\\) to be large and positive in order for the prediction to be correct and confident (i.e. far away from the decision surface). Similarly, if \\(y^{(i)} = -1\\), we need \\(f(x^{(i)})\\) to be a large negative number for our prediction to be confident and correct. Overall, when \\(\hat{\gamma}^{(i)}\\) is large and positive this represents a correct and confident prediction. 

Is this a valid optimization objective to achieve the optimal margin classifier? Unfortunately not. This is because the functional margin is underconstrained. That is, if we replace \\(w\\) with \\(2w\\) and \\(b\\) with \\(2b\\), the decision surface nor the sign of the output will change. To elaborate, if we scale our parameters with a constant \\(c\\), the function \\(c (w^{T}x+b) = 0\\) produces the exact same linear decision surface as \\(w^{T}x+b = 0\\) ([desmos](https://www.desmos.com/calculator/lsy0fpi94g)). Thus, the decision rule is unaffected by scaling of the parameters. However, by replacing \\((w,b)\\) with \\((2w,2b)\\) the functional margin does get multiplied by a factor of 2. This implies that we can make the functional margin as large as we please by simply exploiting our freedom to scale \\(w\\) and \\(b\\), without affecting the actual decision boundary. We will exploit this property later on. 

We also define the functional margin with respect to the training set \\(S\\) as the smallest of the functional margins of the individual training examples. Denoted by \\(\hat{\gamma}\\), this can therefore be written:

\begin{align}
\hat{\gamma} = \min_{i = 1,...,m} \hat{\gamma}^{(i)} \tag{1-4}
\end{align}

### 1.3 Geometric margin
To define the geometric margin, we look at the Euclidean distance from any point \\(\vec{x}^{(i)}\\) of the training set \\(S\\) to the linear decision boundary. We know that the shortest distance between a point and a hyperplane is perpendicular (at 90°) to the plane. This perpendicular vector, pointing between \\(\vec{x}^{(i)}\\) and the decision boundary, is denoted by \\(\vec{r}^{(i)}\\) in Fig. 3. To stay consistent with our notation, we denote the magnitude of this vector as \\(\gamma^{(i)}\\) ( i.e. \\(\|\vec{r}^{(i)}\| = \gamma^{(i)}\\)). Now, taken into account that the normal vector \\(\vec{w}\\) is also perpendicular to the plane, \\(\vec{r}^{(i)}\\) and \\(\vec{w}\\) have the exact same direction. Consequently, we can the express the vector \\(\vec{r}^{(i)}\\) as:

\begin{align}
\vec{r}^{(i)} = \gamma^{(i)} \hat{w} \tag{1-5} \label{1-5}
\end{align}

where \\(\gamma^{(i)}\\) and the unit normal vector \\(\hat{w}\\) capture the correct magnitude and direction of the vector \\(\vec{r}^{(i)}\\), respectively.

__Figure 3: The normal \\(\hat{w}\\) and \\(\vec{r}\\) are parallel__

![Figure 3: The normal vector to the decision boundary and the vector r are parallel](/images/derivation-of-margin.png)

Let us label the point on the hyperplane closest to \\(\vec{x}^{(i)}\\) as \\(\vec{x}^{(i)}_{P}\\). Following the rules of vector addition we get,

\begin{align}
\vec{x}^{(i)}_P = \vec{x}^{(i)} - y^{(i)} \* \vec{r}^{(i)} \tag{1-6} \label{1-6}\\
\end{align}

Note that we multiply \\(\vec{r}^{(i)}\\) by \\(y^{(i)} \in \\{1,-1\\}\\) to capture the desired sign of the operation. In particular, \\(\vec{x}^{(i)}_{P} = \vec{x}^{(i)} + \vec{r}^{(i)}\\) if \\(\vec{x}^{(i)}\\) lays below the decision boundary and \\(\vec{x}^{(i)}_P = \vec{x}^{(i)} - \vec{r}^{(i)}\\) if it lays above the plane. 

Now, substituting \eqref{1-5} into \eqref{1-6} and writing out the unit normal gives,

\begin{align}
\vec{x}^{(i)}_P = \vec{x}^{(i)} - y^{(i)} \* \gamma^{(i)}  \* {\frac{\vec{w}}{\|\vec{w}\|}} \tag{1-7} \label{1-7}\\
\end{align}

Moreover, \\(\vec{x}^{(i)}_P\\) lies on the decision boundary and so satisfies Equation \eqref{1-1}. Substititon therefore yields,

\begin{align}
\vec{w}^{T} \* (\vec{x}^{(i)} - y^{(i)} \* \gamma^{(i)} \* {\frac{\vec{w}}{\|\vec{w}\|}})+b=0 \tag{1-8} \label{1-8}\\
\end{align}

Multiplying out the brackets of Equation \eqref{1-8} yields,

\begin{align}
\vec{w}^{T} \vec{x}^{(i)} - y^{(i)} \* \gamma^{(i)}  \* \frac{\vec{w}^{T}\vec{w}}{\|\vec{w}\|}+b=0 \tag{1-9} \label{1-9}\\
\end{align}

Substituting \\(\vec{w}^{T}\vec{w} = \|\vec{w}\|^{2}\\) into Equation \eqref{1-9} and simplifying gives,

\begin{align}
\vec{w}^{T} \vec{x}^{(i)} - y^{(i)} \* \gamma^{(i)} \* \|\vec{w}\|+b=0 \tag{1-10} \label{1-10}\\
\end{align}

Finally, solving Equation \eqref{1-10} for \\(\gamma^{(i)}\\) gives the geometric margin of an individual training example,

\begin{align}
\gamma^{(i)} =  y^{(i)} \frac{\vec{w}^{T}\vec{x}^{(i)}+b}{\|\vec{w}\|} \tag{1-11} \label{1-11} \\
\end{align}

Clearly, we cannot maximize the margin with respect to every training point. Instead, we maximize the margin with respect to the closest points to the line. If we define \\(\gamma\\) as the distance between the decision boundary and the point closest to it, then

\begin{align}
\gamma = \min_{i = 1, ..., m}\gamma^{(i)} \tag{1-12} \label{1-12}\\
\end{align}

From this, we find the values for the parameters \\(w\\) and \\(b\\) which assure that \\(\gamma\\) is as big as possible. That is, we can extract the optimal geometric margin classifier as:

\begin{equation}
\begin{array}{rrclcl}
\displaystyle \max_{w, b} && \gamma \tag{1-13} \label{1-13} \newline
\textrm{subject to} & \text{ } & y^{(i)} \frac{\vec{w}^{T}\vec{x}^{(i)}+b}{\|\vec{w}\|} \geq \gamma & \forall i = 1, ..., m \newline
\end{array}
\end{equation}

That is, we find the values for the parameters \\(w\\) and \\(b\\) which assure that \\(\gamma\\) is as big as possible, subject to the fact that the distance from the decision boundary to every training example is larger than or equal to the point closest to it. Note that, because we multiply by \\(y^{(i)}\\) in Equation \eqref{1-6}, this maximization procedure forces the training examples to be correctly labelled (i.e. misclassifications will produce negative values of \\(\gamma\\), which opposes the optimization objective). The points closest to the separating hyperplane are called the ___support vectors___. The optimal geometric margin classifier of Equation \eqref{1-13} produces the margin with the maximum width of the band that can be drawn separating the support vectors of the two classes.

Note that the geometric margin is invariant to rescaling of the parameters; i.e., if we replace \\((w, b)\\) with \\((2w, 2b)\\), then the geometric margin does not change. This is due to the normalization parameter in the denominator. This will come in handy later. Specifically, because of this invariance to the scaling of the parameters, when trying to fit \\(w\\) and \\(b\\) to training data, we can impose an arbitrary scaling constraint on the parameters without changing anything important.

### 1.4 Primal form
The above optimization has a non-convex constraint, which we cannot plug into standard optimization software to solve. Instead, we consider an alternative:

\begin{align}
\displaystyle \max_{w, b}  && \hat{\gamma}/\|w\|  \tag{1-14} \label{1-14}\newline
\textrm{subject to} & \text{ } & y^{(i)}(w^{T}x^{(i)} + b) \geq \hat{\gamma} & \forall i = 1, ..., m \newline
\end{align}

Here, we are maximizing \\(\hat{\gamma}/\|w\|\\), subject to the functional margins being at least \\(\hat{\gamma}\\). This is equivalent to the earlier optimization since the geometric and functional margin are related by \\(\gamma = \hat{\gamma}/\|w\|\\). This allow us to get rid of the non-convex constraint. However, we still have a nasty non-convex objective function. Now, taken into account that we can add an arbitrary scaling constraint on \\(w\\) and \\(b\\) without changing anything important, we will introduce the scaling constraint that the functional margin of \\(w\\), \\(b\\) with respect to the training set must be 1:

\begin{align}
\hat{\gamma} = 1 \tag{1-15} \label{1-15}\\
\end{align}

Since multiplying \\(w\\) and \\(b\\) by some constant \\(c\\) results in the functional margin being multiplied by that same constant, this is indeed a scaling constraint, and can be satisfied by rescaling \\(w\\), \\(b\\). Per our earlier discussion, this leaves both the geometric margin and decision boundary unaffected. Thus, plugging this into our problem above, and noting that maximizing \\(1/\|w\|\\) is the same thing as minimizing \\(\|w\|^{2}\\), we now have the following optimization problem:

\begin{equation}
\begin{array}{rrclcl}
\displaystyle \min_{w, b} && \frac{1}{2} \|w\|^{2} \tag{1-16} \label{1-16}\newline
\textrm{subject to} & \text{ } & y^{(i)} (w^{T}x^{(i)} + b) \geq 1 & \forall i = 1, ..., m \newline
\end{array}
\end{equation}

The above is an optimization problem with a convex quadratic objective and only linear constraints. Its solution gives us the optimal margin classifier and can be obtained using quadratic programming.


<font size="6"><b>Chapter 2: The soft-margin primal form</b></font>
### 2.1 Regularization
It is worth noting that the SVM is being implicitly regularized because we minimize \\(\|w\|^{2}\\). This results in relatively small parameters and, in turn, minor changes of the feature values will not cause large changes in the output values - preventing overfitting. This will come in handy once we introduce non-linearity through kernelization. 

The algorithm, however, is still prone to outliers. For example, the figure below demonstrates what happens to the optimal margin classifier once we add a single outlier in the the lower-left region. As can be observed, it causes the decision boundary to migrate while substantially reducing the width of its margin. Taking this to the extreme, if the outlier would lay even further to the lower-left region, the data would become linearly inseperatable.

To make the algorithm  less sensitive to outliers and work for non-linearly separable datasets, the optimization is reformulated using ℓ1-regularization as follows, 

\begin{equation}
\begin{array}{rrclcl}
\displaystyle \min_{w, b, \xi_i} && \frac{1}{2} \|w\|^{2} + C \sum_{i=1}^{m} \xi_i\tag{2-1} \label{2-1} \newline
\textrm{subject to} & \text{ } & y^{(i)} (w^{T}x^{(i)} + b) \geq 1 - \xi_i & \forall i = 1, ..., m \newline
&& \xi_i \geq 0 & \forall i = 1, ..., m \newline
\end{array}
\end{equation}

where \\(xi\\) are non-negative variables that allow training examples to be on the wrong side of their margin as well as the decision boundary and \\(C\\) is a regularization parameter which controls the relative weighting between training accuracy and the mildness (and width) of the margin. The former is achieved with a high value for \\(C\\) (giving a high penalty for misclassification), and the latter with a low value for \\(C\\) (giving a lower penalty for misclassification). The desired value of \\(C\\) is generally found using cross-validation.

### 2.2 Hinge loss function

From Equation \eqref{2-1}, it is worth noting that the constraint \\(y^{(i)} (w^{T}x^{(i)} + b) \geq 1 - \xi_i\\), which is equivalent to \\(\xi_i \geq 1-y^{(i)} (w^{T}x^{(i)} + b)\\), together with \\(\xi_i \geq 0\\), can be written more concisely. Particularly, because we are are minimizing \\(\xi_{(i)}\\), and the smallest value that satisfies the constraint occurs at equality,

\begin{align}
\xi_i = \max(0,1-y^{(i)} (w^{T}x^{(i)} + b)) \tag{2-2} \label{2-2}\\
\end{align}

Hence, the learning problem of Equation \eqref{2-1} is equivalent to the unconstrained optimization problem over \\(w\\),

\begin{align}
\min_{w} \frac{1}{2} \|w\|^{2} + C \sum_{i=1}^{m} \max(0,1-y^{(i)} (w^{T}x^{(i)} + b)) \tag{2-3} \label{2-3}\\
\end{align}

The last part of this equation is called the __hinge loss function__, which is defined as

\begin{align} 
\mathcal{L}(x^{(i)}, y^{(i)}) = max(0,1-y^{(i)} (w^{T}x^{(i)} + b)) \tag{2-4} \label{2-4}
\end{align}

Note that, if a training example has \\(y^{(i)} (w^{T}x^{(i)} + b) < 1\\), then this point violates the margin constraint and therefore contributes to the loss. In contrast, if \\(y^{(i)} (w^{T}x^{(i)} + b) \geq 1\\), then there is no contribution to the loss. Hence, the hinge loss function \\(\mathcal{L}(x^{(i)}, y^{(i)})\\) is so-called piece-wise linear. This is demonstrated in Figure 4.

__Figure 4: Hinge loss function is piece wise linear__

![Figure 4: Hinge loss function is piece wise linear](/images/piece-wise-linear.png)

### 2.3 Solving soft margin primal form
Now, using the hinge loss function we can rewrite the cost function of Equation \eqref{2-3} as,

\begin{align}
\mathcal{C} = \frac{1}{2} \|w\|^{2} + C \sum_{i=1}^{m} \mathcal{L} (x^{(i)}, y^{(i)}) \tag{2-5} \label{2-5}\\
\end{align}

To apply gradient descent to this cost function, the function has to be differentiable. Clearly, because we cannot take the derivative at the point where \\(y^{(i)}(w^{T}x + b) = 1 \\) (see Fig. 4), the hinge loss function is non-differentiable. However, we can calculate the subderivative of the hinge loss function, as such

\begin{align}
\frac{d}{dw} \mathcal{L}(x^{(i)}, y^{(i)}) = \tag{2-6} \label{2-6}
\begin{cases}
-y^{(i)}x^{(i)} & \quad \text{if } y^{(i)} (w^{T}x^{(i)} + b) < 1\newline
0 & \quad \text{else}\newline
\end{cases}
\end{align}

Noting that the derivative of a sum is equal to the sum of its derivatives, we can calculate the gradient of Equation \eqref{2-5} as follows,

\begin{align}
\nabla_{w} \mathcal{C} &= \frac{1}{2}\frac{d}{dw}\|w\|^{2} + C \sum_{i=1}^{m} \frac{d}{dw} {L}(x^{(i)}, y^{(i)}) \tag{2-7} \label{2-7} \newline
&\quad{} = w  + C \sum_{\substack{i = 1 \newline s.t. \newline  y^{(i)}f(x^{(i)}) <1}}^{m} y^{(i)}x^{(i)}
\end{align}

Finally, using the above gradient, we can express the iterative update as,

\begin{align}
w_{(t+1)} &= w_{(t)} - \eta \nabla_{w_{(t)}} \mathcal{C} \tag{2-8} \label{2-8}
\end{align}

where \\(\eta\\) is defined as the learning rate.

Note that the cost function of Equation \eqref{2-5} consists of the sum of two convex functions. Taken in mind that the sum of two convex functions is convex, we can apply gradient descent to find the global optimal parameter values.