---
title: "Support vector machines"
date: 2020-06-16
categories: [Machine learning algorithms]
excerpt: "A thorough explanation of the one of the best off-the-shelf machine learning aglorithms: support vector machine."
header: 
 image: "/images/SVM-image.jpg"
comments: true
mathjax: "true"
classes: wide
sidebar:
  title: "Support vector machines"
  nav: sidebar-SVM
---

<font size="6"><b>  Chapter 1: The maximum margin classifier </b></font>

### 1.1 Decision boundary
The support vector machine (or, abbreviated, "SVM") is a numerical classifier. It is one of the best “off-the-shelf” supervised learning algorithms because of its practical simplicity and computational efficiency (compared to, for instance, artificial neural networks). Given a training set  \\(S = \\{(x^{(i)},y^{(i)})\\, i = 1 \space , ..., \space m \\}\\), where each example belongs to either a positive or negative class \\(y^{(i)} \in \\{-1, +1\\}\\), a SVM training algorithm builds a model that assigns new examples to one category or the other. It does so by means of a linear decision boundary. This linear decision boundary is defined by a line in 2D, a plane in 3D or a hyper plane in higher dimensional feature space, as follows:

\begin{align}
f(x) = w^{T}x+b = 0 \Leftrightarrow \sum_{i = 1}^{n} w_{i}x_{i} + b = 0 \tag{1-1} \label{1-1}
\end{align}

where \\(n\\) is the number of features in our dataset, \\(x \in \mathcal{R}^n\\) is a column vector of feature values,  \\(w \in \mathcal{R}^n\\) is a column vector of weights that is normal to the separating hyperplane and \\(b \in \mathcal{R}\\) is the bias term, without it, the classifier will always go through the origin.

Clearly, a new set of feature values \\( x^{\*} \\) lays on the linear decision boundary if it satisfies Equation \eqref{1-1}. Similarly, it will lay above the decision boundary if \\(f(x^{\*}) > 0\\) and below the decision boundary if \\(f(x^{\*})<0\\). Consequently, the prediction rule, denoted by \\(h(x^{\*})\\), is given by the sign of \\(f(x^{\*})\\). That is, a training example is classified as either a positive or a negative class, as such:

\begin{align}
h(x^{\*})= \tag{1-2} \label{1-2}
\begin{cases}
1 & \quad \text{if } w^{T}x^{\*}+b \geq 0 \newline
-1 & \quad \text{if }w^{T}x^{\*}+b < 0 \newline
\end{cases}
\end{align}

For now, we assume that the data is linearly seperatable.* In such an environment, many possible linear seperators exist. Figure 1 provides two different kinds of linear seperators, dividing the positive class, denoted with crosses, from the negative class, denoted with circles.

\* _We relax this assumption once we introduce the kernelized SVM._

__Figure 1: Different possible linear seperators__

![Figure 1: Different possible linear seperators](/images/possible-decision-boundaries.png)
  
The SVM, however, has a special kind of decision boundary. In contrast to the decision boundaries demonstrated in Figure 1, the SVM assures that the decision boundary is maximally far away from any data point. The distance from the decision boundary to the closest data point determines the so-called __margin__ of the classifier, as shown in Figure 2.

__Figure 2: The maximum margin decision boundary of a SVM__

![Figure 2: The maximum margin decision boundary of a SVM](/images/maximum-margin-decision-boundary.png)

Intuitively, a classifier with a larger margin makes fewer low certainty classification decisions. That is, if we are asked to make a class prediction of a training example close to the decision boundary, this would reflect an uncertain decision. In particular, a small distortion of the feature values could change the outcome of our prediction. By maximizing the margin, we reduce the need to make these kind of low certainty decisions and provide ourselves with some sort of safety margin.

To achieve the goal of maximizing the margin, we first introduce two concepts: the functional margin and the geometric margin. While the latter provides us with a valid optimization objective, the former allows us to simplify it. 

### 1.2 Functional margin
Given a training example \\((x^{(i)}, y^{(i)})\\),  we define the functional margin with respect to the training example as:

\begin{align}
\hat{\gamma}^{(i)} = y^{(i)}(w^{T}x^{(i)}+b) \Leftrightarrow y^{(i)}f(x^{(i)}) \tag{1-3} \label{1-3}
\end{align}

Note that, according to the decision rule of Equation \eqref{1-2}, if \\(y^{(i)} = 1\\) then we need \\(f(x^{(i)})\\) to be large and positive in order for the prediction to be correct and confident (i.e. far away from the decision surface). Similarly, if \\(y^{(i)} = -1\\), we need \\(f(x^{(i)})\\) to be a large negative number for our prediction to be confident and correct. Overall, when \\(\hat{\gamma}^{(i)}\\) is large and positive this represents a correct and confident prediction. 

Is this a valid optimization objective to achieve the optimal margin classifier? Unfortunately not, because  the functional margin is underconstrained. To elaborate, note that if we multiply \\(w\\) and \\(b\\) by some constant \\(c\\), the decision surface nor the sign of the output will change ([check desmos](https://www.desmos.com/calculator/lsy0fpi94g)). This implies that the decision rule is unaffected by scaling of the parameters. However, by replacing \\((w,b)\\) with \\((cw,cb)\\) the functional margin does get multiplied by a factor of \\(c\\). That is, if we scale both our parameters with a constant, the functional margin will be multiplied by that same constant. Thus, we can make the functional margin as large as we please by simply exploiting our freedom to scale \\(w\\) and \\(b\\), without affecting the actual decision boundary. This scaling property clearly implies that the functional margin is an invalid optimization objective. Nevertheless, it will come in handly later on.

We also define the functional margin with respect to the training set \\(S\\) as the smallest of the functional margins of the individual training examples. Denoted by \\(\hat{\gamma}\\), this can therefore be written:

\begin{align}
\hat{\gamma} = \min_{i = 1,...,m} \hat{\gamma}^{(i)} \tag{1-4}
\end{align}

### 1.3 Geometric margin
To define the geometric margin, we look at the Euclidean distance from any point \\(\vec{x}^{(i)}\\) of the training set \\(S\\) to the linear decision boundary. We know that the shortest distance between a point and a hyperplane is perpendicular (at 90°) to the plane. This perpendicular vector, pointing between \\(\vec{x}^{(i)}\\) and the decision boundary, is denoted by \\(\vec{r}^{(i)}\\) in Fig. 3. To stay consistent with our notation, we denote the magnitude of this vector as \\(\gamma^{(i)}\\) ( i.e. \\(\|\vec{r}^{(i)}\| = \gamma^{(i)}\\)). Now, taken into account that the normal vector \\(\vec{w}\\) is also perpendicular to the plane, \\(\vec{r}^{(i)}\\) and \\(\vec{w}\\) have the exact same direction. Consequently, we can the express the vector \\(\vec{r}^{(i)}\\) as:

\begin{align}
\vec{r}^{(i)} = \gamma^{(i)} \hat{w} \tag{1-5} \label{1-5}
\end{align}

where \\(\gamma^{(i)}\\) and the unit normal vector \\(\hat{w}\\) capture the correct magnitude and direction of the vector \\(\vec{r}^{(i)}\\), respectively.

__Figure 3: The normal \\(\hat{w}\\) and \\(\vec{r}\\) are parallel__

![Figure 3: The normal vector to the decision boundary and the vector r are parallel](/images/derivation-of-margin.png)

Let us label the point on the hyperplane closest to \\(\vec{x}^{(i)}\\) as \\(\vec{x}^{(i)}_{P}\\). Following the rules of vector addition we get,

\begin{align}
\vec{x}^{(i)}_P = \vec{x}^{(i)} - y^{(i)} \* \vec{r}^{(i)} \tag{1-6} \label{1-6}\\
\end{align}

Note that we multiply \\(\vec{r}^{(i)}\\) by \\(y^{(i)} \in \\{1,-1\\}\\) to capture the desired sign of the operation. In particular, \\(\vec{x}^{(i)}_{P} = \vec{x}^{(i)} + \vec{r}^{(i)}\\) if \\(\vec{x}^{(i)}\\) lays below the decision boundary and \\(\vec{x}^{(i)}_P = \vec{x}^{(i)} - \vec{r}^{(i)}\\) if it lays above the plane. 

Now, substituting \eqref{1-5} into \eqref{1-6} and writing out the unit normal gives,

\begin{align}
\vec{x}^{(i)}_P = \vec{x}^{(i)} - y^{(i)} \* \gamma^{(i)}  \* {\frac{\vec{w}}{\|\vec{w}\|}} \tag{1-7} \label{1-7}\\
\end{align}

Moreover, \\(\vec{x}^{(i)}_P\\) lies on the decision boundary and so satisfies Equation \eqref{1-1}. Substititon therefore yields,

\begin{align}
\vec{w}^{T} \* (\vec{x}^{(i)} - y^{(i)} \* \gamma^{(i)} \* {\frac{\vec{w}}{\|\vec{w}\|}})+b=0 \tag{1-8} \label{1-8}\\
\end{align}

Multiplying out the brackets of Equation \eqref{1-8} yields,

\begin{align}
\vec{w}^{T} \vec{x}^{(i)} - y^{(i)} \* \gamma^{(i)}  \* \frac{\vec{w}^{T}\vec{w}}{\|\vec{w}\|}+b=0 \tag{1-9} \label{1-9}\\
\end{align}

Substituting \\(\vec{w}^{T}\vec{w} = \|\vec{w}\|^{2}\\) into Equation \eqref{1-9} and simplifying gives,

\begin{align}
\vec{w}^{T} \vec{x}^{(i)} - y^{(i)} \* \gamma^{(i)} \* \|\vec{w}\|+b=0 \tag{1-10} \label{1-10}\\
\end{align}

Finally, solving Equation \eqref{1-10} for \\(\gamma^{(i)}\\) gives the geometric margin of an individual training example,

\begin{align}
\gamma^{(i)} =  y^{(i)} \frac{\vec{w}^{T}\vec{x}^{(i)}+b}{\|\vec{w}\|} \tag{1-11} \label{1-11} \\
\end{align}

Clearly, we cannot maximize the margin with respect to every training point. Instead, we maximize the margin with respect to the closest points to the line. If we define \\(\gamma\\) as the distance between the decision boundary and the point closest to it, then

\begin{align}
\gamma = \min_{i = 1, ..., m}\gamma^{(i)} \tag{1-12} \label{1-12}\\
\end{align}

From this, we find the values for the parameters \\(w\\) and \\(b\\) which assure that \\(\gamma\\) is as big as possible. That is, we can extract the optimal geometric margin classifier as:

\begin{equation}
\begin{array}{rrclcl}
\displaystyle \max_{w, b} && \gamma \tag{1-13} \label{1-13} \newline
\textrm{subject to} & \text{ } & y^{(i)} (\vec{w}^{T}\vec{x}^{(i)}+b) / \|\vec{w}\| \geq \gamma & \forall i = 1, ..., m \newline
\end{array}
\end{equation}

That is, we find the values for the parameters \\(w\\) and \\(b\\) which assure that \\(\gamma\\) is as big as possible, subject to the fact that the distance from the decision boundary to every training example is larger than or equal to the point closest to it. Note that, because we multiply by \\(y^{(i)}\\) in Equation \eqref{1-6}, this maximization procedure forces the training examples to be correctly labelled (i.e. misclassifications will produce negative values of \\(\gamma\\), which opposes the optimization objective). The points closest to the separating hyperplane are called the ___support vectors___. The optimal geometric margin classifier of Equation \eqref{1-13} produces the margin with the maximum width of the band that can be drawn separating the support vectors of the two classes.

Note that the geometric margin is invariant to rescaling of the parameters; i.e., if we replace \\((w, b)\\) with \\((2w, 2b)\\), then the geometric margin does not change. This is due to the normalization parameter in the denominator. This will come in handy later, as it allows us to impose an arbitrary scaling constraint on the parameters without changing anything important.

### 1.4 Primal form
The above optimization has a non-convex constraint, which we cannot plug into standard optimization software to solve. Instead, we consider an alternative:

\begin{align}
\displaystyle \max_{w, b}  && \hat{\gamma}/\|w\|  \tag{1-14} \label{1-14}\newline
\textrm{subject to} & \text{ } & y^{(i)}(w^{T}x^{(i)} + b) \geq \hat{\gamma} & \quad{} \forall i = 1, ..., m \newline
\end{align}

Here, we are maximizing \\(\hat{\gamma}/\|w\|\\), subject to the functional margins being at least \\(\hat{\gamma}\\). This is equivalent to the earlier optimization since the geometric and functional margin are related by \\(\gamma = \hat{\gamma}/\|w\|\\). This allow us to get rid of the non-convex constraint. However, we still have a nasty non-convex objective function. Now, taken into account that we can add an arbitrary scaling constraint on \\(w\\) and \\(b\\) without changing anything important, we will introduce the scaling constraint that the functional margin of \\(w\\), \\(b\\) with respect to the training set must be 1:

\begin{align}
\hat{\gamma} = 1 \tag{1-15} \label{1-15}\\
\end{align}

Since multiplying \\(w\\) and \\(b\\) by some constant \\(c\\) results in the functional margin being multiplied by that same constant, this is indeed a scaling constraint, and can be satisfied by rescaling \\(w\\), \\(b\\). Per our earlier discussion, this leaves both the geometric margin and decision boundary unaffected. Thus, plugging this into our problem above, and noting that maximizing \\(1/\|w\|\\) is the same thing as minimizing \\(\|w\|^{2}\\), we now have the following optimization problem:

\begin{equation}
\begin{array}{rrclcl}
\displaystyle \min_{w, b} && \frac{1}{2} \|w\|^{2} \tag{1-16} \label{1-16}\newline
\textrm{subject to} & \text{ } & y^{(i)} (w^{T}x^{(i)} + b) \geq 1 & \forall i = 1, ..., m \newline
\end{array}
\end{equation}

The above equation is an optimization problem with a convex quadratic objective and only linear constraints. Its solution gives us the optimal margin classifier and can be obtained using QP software.


<font size="6"><b>Chapter 2: The soft-margin primal form</b></font>
### 2.1 Regularization
It is worth noting that the SVM is being implicitly regularized because we are minimizizing \\(\|w\|^{2}\\). This results in relatively small parameters. In turn, small changes of feature values will not result in large changes in the output values. Hence, preventing overfitting. This will come in handy once we introduce non-linearity through kernelization. 

The algorithm, however, is still prone to outliers. For example, the figure below demonstrates what happens to the optimal margin classifier once we add a single outlier in the the lower-left region. As can be observed, it causes the decision boundary to migrate while substantially reducing the width of its margin. Taking this to the extreme, if the outlier would lay even further to the lower-left region, the data would become linearly inseperatable.

__Figure 4: Effect of outliers on maximum margin classifier__

![Figure 4: Effect of outlier on maximum margin classifier](/images/Low-C-versus-High-C.png)

To make the algorithm  less sensitive to outliers and work for non-linearly separable datasets, the optimization is reformulated using ℓ1-regularization as follows, 

\begin{equation}
\begin{array}{rrclcl}
\displaystyle \min_{w, b, \xi_i} && \frac{1}{2} \|w\|^{2} + C \sum_{i=1}^{m} \xi_i\tag{2-1} \label{2-1} \newline
\textrm{subject to} & \text{ } & y^{(i)} (w^{T}x^{(i)} + b) \geq 1 - \xi_i & \forall i = 1, ..., m \newline
&& \xi_i \geq 0 & \forall i = 1, ..., m \newline
\end{array}
\end{equation}

where \\(\xi_{i}\\) are non-negative variables that allow training examples to be on the wrong side of their margin as well as the decision boundary and \\(C\\) is a regularization parameter which controls the relative weighting between training accuracy and the mildness (and width) of the margin. The former is achieved with a high value for \\(C\\) (giving a high penalty for misclassification), and the latter with a low value for \\(C\\) (giving a lower penalty for misclassification). The desired value of \\(C\\) is generally found using cross-validation.

### 2.2 Hinge loss function

From Equation \eqref{2-1}, it is worth noting that the constraint \\(y^{(i)} (w^{T}x^{(i)} + b) \geq 1 - \xi_i\\), which is equivalent to \\(\xi_{i} \geq 1-y^{(i)} (w^{T}x^{(i)} + b)\\), together with \\(\xi_i \geq 0\\), can be written more concisely. Particularly, because we are are minimizing \\(\xi_{(i)}\\), and the smallest value that satisfies the constraint occurs at equality,

\begin{align}
\xi_i = \max(0,1-y^{(i)} (w^{T}x^{(i)} + b)) \tag{2-2} \label{2-2}\\
\end{align}

Hence, the learning problem of Equation \eqref{2-1} is equivalent to the unconstrained optimization problem over \\(w\\),

\begin{align}
\min_{w} \frac{1}{2} \|w\|^{2} + C \sum_{i=1}^{m} \max(0,1-y^{(i)} (w^{T}x^{(i)} + b)) \tag{2-3} \label{2-3}\\
\end{align}

The last part of this equation is called the __hinge loss function__, which is defined as

\begin{align} 
\mathcal{L}(x^{(i)}, y^{(i)}) = \max(0,1-y^{(i)} (w^{T}x^{(i)} + b)) \tag{2-4} \label{2-4}
\end{align}

Note that, if a training example has \\(y^{(i)} (w^{T}x^{(i)} + b) < 1\\), then this point violates the margin constraint and therefore contributes to the loss. In contrast, if \\(y^{(i)} (w^{T}x^{(i)} + b) \geq 1\\), then there is no contribution to the loss. Hence, the hinge loss function \\(\mathcal{L}(x^{(i)}, y^{(i)})\\) is so-called piece-wise linear. This is demonstrated in Figure 4.

__Figure 4: Hinge loss function is piece wise linear__

![Figure 4: Hinge loss function is piece wise linear](/images/piece-wise-linear.png)

### 2.3 Solving soft margin primal form
Now, using the hinge loss function we can rewrite the cost function of Equation \eqref{2-3} as,

\begin{align}
\mathcal{C} = \frac{1}{2} \|w\|^{2} + C \sum_{i=1}^{m} \mathcal{L} (x^{(i)}, y^{(i)}) \tag{2-5} \label{2-5}\\
\end{align}

To apply gradient descent to this cost function, the function has to be differentiable. Clearly, because we cannot take the derivative at the point where \\(y^{(i)}(w^{T}x + b) = 1 \\) (see Fig. 4), the hinge loss function is non-differentiable. However, we can calculate the subderivative of the hinge loss function, as such

\begin{align}
\frac{d}{dw} \mathcal{L}(x^{(i)}, y^{(i)}) = \tag{2-6} \label{2-6}
\begin{cases}
-y^{(i)}x^{(i)} & \quad \text{if } y^{(i)} (w^{T}x^{(i)} + b) < 1\newline
0 & \quad \text{else}\newline
\end{cases}
\end{align}

Noting that the derivative of a sum is equal to the sum of its derivatives, we can calculate the gradient of Equation \eqref{2-5} as follows,

\begin{align}
\nabla_{w} \mathcal{C} &= \frac{1}{2}\frac{d}{dw}\|w\|^{2} + C \sum_{i=1}^{m} \frac{d}{dw} {L}(x^{(i)}, y^{(i)}) \tag{2-7} \label{2-7} \newline
&\quad{} = w  + C \sum_{\substack{i = 1 \newline s.t. \newline  y^{(i)}f(x^{(i)}) <1}}^{m} y^{(i)}x^{(i)}
\end{align}

Finally, using the above gradient, we can express the iterative update as,

\begin{align}
w_{(t+1)} &= w_{(t)} - \eta \nabla_{w_{(t)}} \mathcal{C} \tag{2-8} \label{2-8}
\end{align}

where \\(\eta\\) is defined as the learning rate.

Note that the cost function of Equation \eqref{2-5} consists of the sum of two convex functions. Taken in mind that the sum of two convex functions is convex, we can apply gradient descent to find the global optimal parameter values.

<font size="6"><b>  Chapter 3: Dual form </b></font>

### 3.1 Dual optimization
To be able to efficiently work with non-linear data, we need to apply the so-called kernel trick (see Section 4). The primal problem is not ready to be kernelised, however, because its dependence is not only on inner products between data-vectors. To achieve this, we transform to the dual formulation by first writing the primal soft-margin optimization problem of Equation \eqref{2-1} using a generalized Lagrangian,

\begin{align} 
L(w, b, \xi, \alpha, \beta) = \underbrace{\frac{1}{2}\|w\|^{2} + C \sum_{i}^{m} \xi_{i}}_\textrm{Primal objective} + \underbrace{\sum_{i}^{m} \alpha_{i} [(1 - \xi_{i}) - y^{(i)} (w^{T} x^{(i)} + b)]}_\textrm{First set of ineq. constraints} + \underbrace{\sum_{i}^{m} \beta_{i} (-\xi_{i})}_\textrm{Second set of ineq. constraints} \tag{3-1} \label{3-1}
\end{align}

where \\(\alpha_i\\) and \\(\beta_i\\) are Lagrange multipliers that capture the inequality constraints of the regularized primal optimization problem. Now, consider the quantity,

\begin{align}
\theta_P(w,b,\xi): \max_{\substack{\alpha, \beta \\ \alpha_i \geq 0, \beta_i \geq 0}} \quad{} L(w, b, \xi, \alpha, \beta)  \tag{3-2} \label{3-2}
\end{align}

where the subscript \\(P\\) stands for primal. Note that, if the constraint \\(\xi_i \geq 0\\) is violated (i.e. \\(\xi_i < 0\\) in the above maximization problem, then the adversary is free to inflate the value of \\(\beta_i\\) when maximizing the Lagragian with respect to beta. In turn, causing the sum in the last term of \eqref{3-1} to become \\(+\infty\\). Similarly, if the margin constraint \\(y^{(i)}(w^{T}x^{(i)} + b) \geq 1-\xi_i\\) is violated (i.e. \\([(1 - \xi_i) - y^{(i)}(w^{T}x^{(i)} + b)] > 0\\)), then the adversary is free to set \\(\alpha_{i}\\) to a very large positive value when maximizing the Lagrangian with respect to alpha. In turn, causing the  sum in the third term of Equation \eqref{3-1} to become \\(+\infty\\). On the other hand, taken into account that the alphas and betas are constrained to be non-negative, if both the constraints hold, the best values for \\((\alpha_i, \beta_i)\\) that the adversary can find is \\((0,0)\\). Thus, if non of the constraints are violated for all \\(i = 1, ..., m\\), the last two terms of equation \eqref{3-1} disappear. It follows that, for a given \\(w\\), \\(b\\) and \\(\xi\\), we have

\begin{align}
\theta_P(w,b,\xi) = \tag{3-3} \label{3-3}
\begin{cases}
\frac{1}{2}\|w\|^{2} + C \sum_{i}^{m} \xi_i & \text{if primal constraints hold} & \forall i = 1, ..., m \newline
\infty & \text{otherwise} \newline
\end{cases}
\end{align}

Thus, if the constraints do not hold, the adversary can penalize us by making the objective function arbitrarily large. Consequently, when minimizing Equation \eqref{3-3} with respect to the parameters \\((w, b, \xi)\\), the algorithm will always select parameters which satisfy the constraint. Hence, the minimization problem 

\begin{align}
\min_{w, b, \xi} \theta_P(w,b, \xi)  =  \min_{w, b, \xi} \max_{\substack{\alpha, \beta \\ \alpha_i \geq 0, \beta_i \geq 0}} L(w, b, \xi, \alpha, \beta)  \tag{3-4} \label{3-4}
\end{align}

is equivalent to the primal problem of Equation \eqref{2-1}. Now, we can also work the other way around by first minimizing with respect to the parameters \\((w, b, \xi)\\) and subsequently maximizing with respect to \\((\alpha, \beta)\\). That is, we define

\begin{align}
\theta_{D} (\alpha,\beta):  \min_{w, b, \xi} L(w, b, \xi, \alpha, \beta) \tag{3-5} \label{3-5}\\
\end{align}

where the subscript \\(D\\) stands for dual. Then the dual optimization problem is given by

\begin{align}
\max_{\substack{\alpha,\beta \newline \alpha_i \geq 0, \beta_i \geq 0}} \theta_D(\alpha,\beta) = \max_{\substack{\alpha,\beta \newline \alpha_i \geq 0, \beta_i \geq 0}} \min_{w, b, \xi} L(w, b, \xi, \alpha, \beta) \tag{3-6} \label{3-6} \\
\end{align}

Taken into account that “max min”  is always  less than or equal to the “min max" of a function, we have the following inequality between the outcomes of the primal and the dual optimization problem,

\begin{align}
\max_{\substack{\alpha,\beta \newline \alpha_i \geq 0, \beta_i \geq 0}} \theta_D(\alpha,\beta) \leq  \min_{w, b} \theta_P(w,b) \tag{3-7} \label{3-7}\\
\end{align}

That is, often __duality gaps__ arise between the outcomes of the primal and dual problem. However, it can be shown that if the objective function is quadratic convex and the constraints are affine (that is, linear including a bias term), then the duality gap is always zero\*, provided one of the primal or dual problems is feasibile. Taken into account that the objective function and constraints of the primal problem as shown in Equation \eqref{3-4} is QP, this implies that strong duality holds and the duality gap is zero.  That is, there exists \\(w^{\*}\\), \\(b^{\*}\\), \\(\xi^{\*}\\), \\(\alpha^{\*}\\) and \\(\beta^{\*}\\) such that \\(w^{\*}\\), \\(b^{\*}\\) and \\(\xi^{\*}\\) are the solution to the primal problem and \\(\alpha^{\*}\\) and \\(\beta^{\*}\\) are the solution to the dual problem and both outcomes are identical. This allows us to solve the dual problem of Equation \eqref{3-6} instead of the primal problem. Moreover, \\(w^{\*}\\), \\(b^{\*}\\), \\(\xi^{\*}\\), \\(\alpha^{\*}\\) and \\(\beta^{\*}\\) satisfy the __Karush-Kuhn-Tucker (KKT) conditions__. We will return to those conditions in Section 3.2. 

\*_Note that there are many other similar results that guarantee a zero duality gap._


### 3.2 Solving dual form
Taken into account the zero duality gap, we wish to find a solution to the dual optimization problem of Equation \eqref{3-6}. Recall that the dual optimization problem is defined as,

\begin{align}
\max_{\substack{\alpha,\beta \\ a_i \geq 0, \beta_i \geq 0}} \min_{w, b, \xi} \quad{} \frac{1}{2}\|w\|^{2} + C\sum_{i}^{m} \xi_i + \sum_{i}^{m} \alpha_i [(1 - \xi_i) - y^{(i)} (w^{T}x^{(i)} + b)] + \sum_{i}^{m} \beta_i (-\xi_i)  \tag{3-8} \label{3-8}
\end{align}

To find the solution of the problem, we fix \\(a\\) and \\(\beta\\) and minimize \\(L(w, b, \xi, \alpha, \beta)\\) with respect to \\(w\\), \\(b\\) and \\(\xi\\). This can be done by setting the respective derivatives to zero. Subsequently, we can maximize the reduced Lagrangian with respect to \\(\alpha\\) and \\(\beta\\).

Taking the derivative of the Lagrangian with respect to \\(w\\) and setting it to zero gives,

\begin{align}
\frac{\delta}{\delta w} L(w, b, \xi, \alpha, \beta) = w - \sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} = 0 \Rightarrow w = \sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} \tag{3-9} \label{3-9}
\end{align}

Similarly, taking the derivative of the Lagrangian with respect to the bias and setting it to zero gives,

\begin{align}
\frac{\delta}{\delta b}L(w, b, \xi, \alpha, \beta) = - \sum_{i=1}^{m} \alpha_iy^{(i)} = 0 \Rightarrow \sum_{i=1}^{m} \alpha_iy^{(i)} = 0 \tag{3-10}  \label{3-10}
\end{align}

Finally, taking the derivative of the Lagrangian with respect to the slack variable and setting it to zero gives,

\begin{align}
\frac{\delta}{\delta \xi}L(w, b, \xi, \alpha, \beta) = C-\alpha_i-\beta_i = 0 \Leftrightarrow \beta_i = C - \alpha_i \tag{3-11} \label{3-11}
\end{align}

Given the dual constraint that \\(\beta_i \geq 0\\), the above expression leads to a constraint that \\(C-\alpha_i \geq 0\\) or \\(\alpha_i \leq C\\). Furthermore, given the dual constraint that \\(\alpha_i \geq 0\\), we end up with the following constraint on alpha,

\begin{align}
0\leq \alpha_i \leq C, \quad{} \forall i = 1,...,m \tag{3-12} \label{3-12}
\end{align}

Now, substituting Equations \eqref{3-9} - \eqref{3-11} back into \\(L(w, b, \xi, \alpha, \beta)\\) and simplifying allows us to rewrite the Lagrangian only in terms of alpha. 

\begin{align}
L(a) = \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j = 1}^m \alpha_i \alpha_j y^{(i)}y^{(j)} (x^{(i)})^{T} x^{(j)} \tag{3-13} \label{3-13}
\end{align}

Recall the constraints on alpha that we obtained in Equation \eqref{3-10} and \eqref{3-12}. Putting this together with the above reduced Lagrangian, we obtain the dual optimization problem:

\begin{equation}
\begin{array}{rrclcl}
\displaystyle \max_{\alpha} & \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)}y^{(j)} \langle x^{(i)},x^{(j)} \rangle \tag{3-14} \label{3-14} \newline
\textrm{subject to} &0 \leq \alpha_i \leq C \quad{} \forall i = 1, ..., m \newline
& \sum_{i=1}^{m}\alpha_iy^{(i)} = 0  \newline
\end{array}
\end{equation}

Note that we have replaced \\((x^{(i)})^{T}x^{(j)}\\) with \\(\langle x^{(i)},x^{(j)} \rangle\\). This is just the definition of the inner product, and will be useful for the kernelized SVM.


### 3.3 KKT Conditions and strong duality
If for some \\(w^{\*}\\), \\(b^{\*}\\), \\(\xi^{\*}\\), \\(\alpha^{\*}\\) and \\(\beta^{\*}\\) strong duality holds, then the KKT conditions follow. Moreover, if the KKT conditions hold, then  \\(w^{\*}\\), \\(b^{\*}\\), \\(\xi^{\*}\\), \\(\alpha^{\*}\\) and \\(\beta^{\*}\\) are primal and dual solutions. Thus, the KKT conditions are always sufficient but necessary under strong duality. The KKT conditions of the given primal and dual problem are as follows:

__1:__ Primal feasibility: the primal constraints hold, i.e.
   
\\(\quad{} y^{(i)} ((w^{\*})^{T}x^{(i)} + b^{\*}) \geq 1 - \xi_i^{\*}  \quad{} \forall i = 1, ..., m \tag{1.1} \label{1.1}\\)
    
\\(\quad{} \xi_i^{\*} \geq 0 \quad{} \forall i = 1, ..., m \tag{1.2} \label{1.2} \\)


__2:__ Dual feasibilitiy: the dual constraints hold, i.e.
    
\\(\quad{} a^{\*} \geq 0 \tag{2.1} \label{2.1} \\)
    
\\(\quad{} \beta^{\*} \geq 0 \tag{2.2} \label{2.2}\\)


__3:__ Complimentary slackness, i.e.
    
* \\(\quad{} \alpha_{i}^{\*} \\{(1 - \xi_{i}^{\*}) - y^{(i)} ( (w^{\*})^{T}x^{(i)} + b^{\*}) \\} = 0 \quad{} \forall i = 1, ..., m  \tag{3.1} \label{3.1} \\)
    
* \\(\quad{} \beta_{i}^{\*} ( \text{-} \xi_{i}^{\*} ) = 0 \quad{} \forall i = 1, ..., m  \tag{3.2} \label{3.2} \\)

__4:__ Stationarity: the gradient with respect to \\(w\\), \\(b\\) and \\(\xi\\) is zero, i.e.

* \\(\tag{Test} \\) \\( \quad{} \frac{\delta}{\delta w}L(w^{\*}, b^{\*}, \xi^{\*}, \alpha^{\*}, \beta^{\*}) = 0 \tag{4.1} \label{4.1} \\)
    
* \\( \quad{} \frac{\delta}{\delta b}L(w^{\*}, b^{\*}, \xi^{\*}, \alpha^{\*}, \beta^{\*}) = 0 \tag{4.2} \label{4.2}\\)
    
* \\(\quad{} \frac{\delta}{\delta \xi}L(w^{\*}, b^{\*}, \xi^{\*}, \alpha^{\*}, \beta^{\*}) = 0 \tag{4.3} \label{4.3}\\)

From these conditions, we can extract some usefull information. In particular,

* If \\(\alpha_{i}^{\*} = 0 \quad{} \xrightarrow{\text{Eq. } \eqref{3-11}} \quad{}  \beta_{i}^{\*} = C \quad{} \xrightarrow{\text{Con. } \eqref{3.2}} \quad{} \quad{} \xi_{i}^{\*} = 0 \quad{}  \xrightarrow{\text{Con. } \eqref{1.1}} \quad{}  y^{(i)} ((w^{*})^{T}x^{(i)} + b^{\*}) \geq 1 \\)
* If \\(\alpha_{i}^{\*} = C \quad{} \xrightarrow{\text{Eq. } \eqref{3-11}} \quad{}  \beta_{i}^{\*} = 0 \quad{}  \xrightarrow{\text{Con. } \eqref{1.1} \text{ & } \eqref{1.2}} \quad{}  y^{(i)} ((w^{\*})^{T}x^{(i)} + b^{\*}) \leq 1\\)
* If \\(0 < \alpha_{i}^{\*} < C \quad{}  \xrightarrow{\text{Eq. } \eqref{3-11}} \quad{}  \beta_{i}^{\*} \in (0,C) \quad{} \xrightarrow{\text{Con.} \eqref{3.2}} \quad{}  \xi_{i}^{\*} = 0 \quad{}  \xrightarrow{\text{Con. } \eqref{3.1}} \quad{}  y^{(i)} ((w^{\*})^{T}x^{(i)} + b^{\*}) = 1\\)

Overall, it follows that for \\(a_{i}^{\*} \in (0,C)\\) we have \\(y^{(i)} ((w^{\*})^{T} x + b^{\*}) = 1\\). In other words, the non-negative alpha's ranging between \\(0\\) and \\(C\\) have a margin distance from the seperating hyperplane and, thus, must be the support vectors. The direct consequence of this is that we are in a position to easily identify the support vectors from the training set. At the same time, it provides an indicator regarding whether or not the algorithm is overfitting (fewer support vectors indicates a lower VC dimensionality).

### 3.4 Classification
Suppose we fit the model's parameters to a training set, extracted the optimal value \\(\alpha^{\*}\\) and now wish to make a prediciton \\(\hat{y}\\) at a new point \\(x\\). This would require us to calculate the sign of \\((w^{\*})^{T}x+b^{\*}\\). This quantity, however, depends on \\(w^{\*}\\) while the solution to the dual problem gives us the optimal values of alpha. Luckily, we can use Equation \eqref{3-9} to express \\(w^{\*}\\) in terms of \\(\alpha^{\*}\\). In particular, to make a prediction we can take the sign of

\begin{align}
(w^{\*})^{T}x+b^{\*} \Rightarrow (\sum_{i=1}^{m} \alpha_i^{\*} y^{(i)} x^{(i)})^{T}x+b^{\*} \Rightarrow \sum_{i=1}^{m} \alpha_i^{\*} y^{(i)} \langle x^{(i)}, x \rangle + b^{\*} \tag{3-15} \label{3-15}
\end{align}

Hence, if we’ve found the \\(\alpha_i^{\*}\\)'s, we have to calculate quantities that depend only on the inner products between our new point \\(x\\) and the examples in the training set. Taken into account that the majority of these \\(\alpha_i^{\*}\\)'s are zero, many of the terms in the sums above to calculate  will be zero. In turn, we only have to take the inner products between the new point \\(x\\) and a small set of support vectors to do predictions. 

Now, the only thing left to do is to find the optimal value of the bias in terms of alpha. To do so, we consider a support vector (i.e. an unbound example with \\(0 < a_{i}^{\*} < C\\)). According to the complimentary slackness condtions of the previous section, this vector has \\(y^{(i)} ((w^{\*})^{T} x^{(i)}+ b^{\*}) = 1\\). Rewriting this in terms of \\(b^{\*}\\) and taking into account that \\(y^{(i)} \in \{-1,+1\}\\) we get,

\begin{align}
b^{\*} = \tag{3-16} \label{3-16}
\begin{cases}
1 - (w^{\*})^{T} x^{(i)} & \quad \text{if } y^{(i)} = 1 \newline
\text{-} 1 - (w^{\*})^{T} x^{(i)}  & \quad \text{if } y^{(i)} = -1 \newline
\end{cases}
\end{align}

Or more consicely,

\begin{align}
b^{\*} = y^{(i)} - (w^{\*})^{T}x^{(i)} \tag{3-17} \label{3-17} \\
\end{align}

From a numerical stability standpoint, and in particular when taking into account the stopping criteria, the actual value of the margin may not be exactly equal to 1, so generally one averages over the \\( b^{\*} \\) 's of all support vectors when performing the calculation. If we denote \\( S = \\{i\|\alpha_{i}^{\*}>0\\} \\) as the set of support vectors, then

\begin{align}
b^{\*} = \frac{1}{\|S\|} \sum_{i \in S} [y^{(i)} - (w^{\*})^{T}x^{(i)}] \tag{3-18} \label{3-18}
\end{align}

where \\(\|S\|\\) denotes the number of support vectors in the set. We already derived the optimal value of \\(w^{\*}\\) above, so we could stop here. However, for the purpose of implementing kernelization, we can substitute Equation \eqref{3-9} into the above equation and rewrite it as follows

\begin{align}
b^{\*} = \frac{1}{\|S\|} \sum_{i \in S}  [y^{(i)} - (\sum_{j=1}^{m} \alpha_j^{\*} y^{(j)} x^{(j)})^{T}x^{(i)}] \tag{3-19} \label{3-19}
\end{align}

Now, by distributing the transpose, we can rewrite this using inner products as follows

\begin{align}
b^{\*} = \frac{1}{\|S\|} \sum_{i \in S}  [y^{(i)} - \sum_{j=1}^{m} \alpha_j^{\*} y^{(j)} \langle x^{(j)}, x^{(i)}\rangle] \tag{3-20} \label{3-20}
\end{align}

Note that, just like the objective function in Equation \eqref{3-14}, the optimal bias term as given in Equation \eqref{3-20} depends on the inner product of the training examples with the support vectors. In contrast, the sum in Equation \eqref{3-15} depends on the inner product between the training examples with a _new_ input value \\(x\\). Despite this subtle difference, we are now in a position to kernelize the dual SVM problem. 